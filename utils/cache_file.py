import json
import nltk
from pathlib import Path
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords

## Generated by Gemini
# Download only the resources needed: WordNet for definitions and stopwords
# We are no longer downloading any corpora for word frequency.
nltk.download("wordnet", quiet=True)
nltk.download("stopwords", quiet=True)

# Define the directory where the script is located
SCRIPT_DIR = Path(__file__).parent

# Cache file paths (now guaranteed to be in the script's directory)
WORDS_CACHE_FILE = SCRIPT_DIR / "common_words_10k.json"
DEFINITIONS_CACHE_FILE = SCRIPT_DIR / "common_word_definitions.json"


def create_common_word_caches(top_n=10000, min_length=5, external_files=None):
    """
    Creates cache files for the top `top_n` most common alphabetic words
    from the external files, excluding stopwords, along with their WordNet definitions.
    """

    if not external_files:
        print("ERROR: No external files provided. Please specify a list of file paths.")
        return

    print(f"Extracting words from external files (filtering ≥{min_length} chars, no stopwords)...")

    # --- Read and Combine External Files ---
    all_words = []
    for file_path in external_files:
        # file_path is now an absolute Path object, which is more reliable
        print(f"Reading words from {file_path.name} to maintain rank order...")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # Assuming one word per line, already lowercased
                words = [line.strip().lower() for line in f if line.strip()]
                all_words.extend(words)
        except FileNotFoundError:
            # If the absolute path still fails, the file likely doesn't exist
            print(f"ERROR: File not found at the calculated path: {file_path}. Skipping.")
            continue
    # ---------------------------------------

    if not all_words:
        print("No words were successfully read from the specified files. Aborting.")
        return

    # Get English stopwords
    stop_words = set(stopwords.words('english'))
    print(f"Loaded {len(stop_words)} stopwords to filter out")

    # Create an ordered list of unique, filtered words (maintaining rank)
    common_words_set = set()
    common_words = []

    for word in all_words:
        # Check if word is new, purely alphabetic, not a stop word, and meets min length
        if (word not in common_words_set and
            word.isalpha() and
            word not in stop_words and
            len(word) >= min_length):

            common_words.append(word)
            common_words_set.add(word)

    # Truncate to top_n words
    common_words = common_words[:top_n]

    print(f"Found {len(common_words)} unique, filtered words from external files.")

    # Fetch definitions (using NLTK's WordNet)
    print("Fetching WordNet definitions...")

    definitions = {}
    for i, word in enumerate(common_words):
        if i % 1000 == 0:
            print(f"  Processing {i}/{len(common_words)}...")

        synsets = wn.synsets(word)
        if synsets:
            # Store up to the first 3 definitions
            definitions[word] = [s.definition() for s in synsets[:3]]

    print(f"Definitions found for {len(definitions)} of {len(common_words)} words")

    # Keep only words that have definitions
    final_words = [w for w in common_words if w in definitions]

    # Save words cache
    print(f"Saving {len(final_words)} words to {WORDS_CACHE_FILE.name}...")
    with WORDS_CACHE_FILE.open("w", encoding="utf-8") as f:
        json.dump(final_words, f, indent=2)

    # Save definitions cache
    print(f"Saving {len(definitions)} definitions to {DEFINITIONS_CACHE_FILE.name}...")
    with DEFINITIONS_CACHE_FILE.open("w", encoding="utf-8") as f:
        json.dump(definitions, f, indent=2)

    print("\n✓ Cache creation complete!")
    print(f"  Words saved: {len(final_words)}")
    print(f"  Definitions saved: {len(definitions)}")


if __name__ == "__main__":
    # Define the names of the external files.
    EXTERNAL_WORD_LIST_NAMES = [
        "google-10000-english-usa-no-swears-medium.txt",
        "google-10000-english-usa-no-swears-long.txt"
    ]

    # Construct the full path for each file by joining the script's directory (SCRIPT_DIR)
    # with the file name. This ensures the correct path is always used.
    EXTERNAL_WORD_LIST_PATHS = [
        SCRIPT_DIR / name for name in EXTERNAL_WORD_LIST_NAMES
    ]

    # Run the cache creation function
    create_common_word_caches(
        top_n=10000,
        min_length=5,
        external_files=EXTERNAL_WORD_LIST_PATHS
    )